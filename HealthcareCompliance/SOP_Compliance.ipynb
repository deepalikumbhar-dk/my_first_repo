{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zs245e2f7juxky7t7qew",
   "authorId": "2443334921470",
   "authorName": "ANALYTICSINNOVATORS",
   "authorEmail": "aakarsh.sharma@jadeglobal.com",
   "sessionId": "d72039d6-16df-4594-8b8b-ea7528d7c57c",
   "lastEditTime": 1757852507962
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "60ecd66a-6ebc-4b6d-8d69-79544aaf693f",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\nfrom snowflake.cortex import complete\nimport docx\nfrom datetime import datetime\nimport os\nimport re\n\n# Setup Snowflake session\ndef create_snowflake_session():\n    connection_parameters = {\n        \"account\": \"IW69072\",\n        \"user\": \"AnalyticsInnovators\",\n        \"password\": \"AnalyticsInnovators@123\",\n        \"role\": \"ACCOUNTADMIN\",\n        \"warehouse\": \"COMPUTE_WH\",\n        \"database\": \"SOP_COMPLIANCE\",\n        \"schema\": \"PUBLIC\"\n    }\n    return Session.builder.configs(connection_parameters).create()\n\n# Download files from internal stage to temp and return paths\ndef download_files(session):\n    temp_dir = \"temp_stage_files\"\n    os.makedirs(temp_dir, exist_ok=True)\n\n    doc_stage_path = \"@my_stage/Synthetic_SOP_Compliant.docx\"\n    prompt_stage_path = \"@my_stage/gxP_prompts.txt\"\n    score_prompt_stage_path = \"@my_stage/score_prompts.txt\"\n\n    doc_temp_path = os.path.join(temp_dir, \"Synthetic_SOP_Compliant.docx\")\n    prompt_temp_path = os.path.join(temp_dir, \"gxP_prompts.txt\")\n    score_prompt_temp_path = os.path.join(temp_dir, \"score_prompts.txt\")\n\n    session.file.get(doc_stage_path, temp_dir)\n    session.file.get(prompt_stage_path, temp_dir)\n    session.file.get(score_prompt_stage_path, temp_dir)\n\n    return doc_temp_path, prompt_temp_path, score_prompt_temp_path\n\n# Parse SOP document into sections\ndef parse_document(doc_path):\n    doc = docx.Document(doc_path)\n    sections = {}\n    current_section = \"Header\"\n    sections[current_section] = []\n\n    for para in doc.paragraphs:\n        text = para.text.strip()\n        if not text:\n            continue\n        if text.startswith(\"##\") or text in [\n            \"Revision History\", \"Introduction\", \"Purpose\", \"Scope\",\n            \"Responsibilities\", \"Definitions\", \"Procedure\", \"References\", \"Approvals\"\n        ]:\n            current_section = text.strip()\n            sections[current_section] = []\n        else:\n            sections.setdefault(current_section, []).append(text)\n\n    return {k: \"\\n\".join(v) for k, v in sections.items()}\n\n# Load prompts from file with section headers\ndef load_prompts(prompt_path):\n    with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n    pattern = r\"([\\w\\s]+)\\s*\\(\\s*(.*?)\\s*\\)\"\n    matches = re.findall(pattern, content, re.DOTALL)\n    return {section.strip(): prompt.strip() for section, prompt in matches}\n\n# Analyze document using Cortex\ndef analyze_document(session, doc_name, prompts, score_prompts, sections):\n    results = []\n    timestamp = datetime.utcnow().isoformat()\n\n    for section, analysis_prompt in prompts.items():\n        section_text = sections.get(section, \"\")\n        full_analysis_prompt = f\"Section Content:\\n{section_text}\\n\\nPrompt:\\n{analysis_prompt}\"\n\n        ai_response = complete(\n            model=\"claude-3-5-sonnet\",\n            prompt=full_analysis_prompt,\n            session=session\n        )\n\n        score_prompt = score_prompts.get(section, \"\")\n        full_score_prompt = f\"AI Response:\\n{ai_response}\\n\\nBased on the following scoring rules, assign severity and score:\\n{score_prompt}\"\n\n        score_response = complete(\n            model=\"claude-3-5-sonnet\",\n            prompt=full_score_prompt,\n            session=session\n        )\n\n        severity_match = re.search(r\"Severity\\s*[:\\-]?\\s*(Critical|Major|Minor)\", score_response, re.IGNORECASE)\n        score_match = re.search(r\"Score\\s*[:\\-]?\\s*(\\d+)\", score_response)\n\n        severity = severity_match.group(1).capitalize() if severity_match else \"Minor\"\n        score = int(score_match.group(1)) if score_match else 1\n\n        results.append((doc_name, section, analysis_prompt, severity, score, ai_response, timestamp))\n\n    return results\n\n# Save results to Snowflake\ndef save_results(session, results):\n    df = session.create_dataframe(results, schema=[\n        \"document_name\", \"section\", \"issue\", \"severity\", \"score\", \"ai_response\", \"timestamp\"\n    ])\n    df.write.mode(\"append\").save_as_table(\"sop_compliance_results\")\n\n# Calculate and print compliance score\ndef print_compliance_score(results):\n    total_score = sum([r[4] for r in results])\n    max_score = len(results) * 3\n    compliance_percent = 100 - int((total_score / max_score) * 100)\n    print(f\"Compliance Score: {compliance_percent}/100\")\n\n# Main execution\ndef main():\n    session = create_snowflake_session()\n    doc_path, prompt_path, score_prompt_path = download_files(session)\n\n    doc_name = \"Synthetic_SOP_Compliant.docx\"\n    existing = session.table(\"sop_compliance_results\").filter(col(\"document_name\") == doc_name)\n\n    if existing.count() > 0:\n        print(\"Document already analyzed. Replacing old results...\")\n        session.sql(f\"\"\"\n            DELETE FROM sop_compliance_results\n            WHERE document_name = '{doc_name}'\n        \"\"\").collect()\n\n    sections = parse_document(doc_path)\n    prompts = load_prompts(prompt_path)\n    score_prompts = load_prompts(score_prompt_path)\n    results = analyze_document(session, doc_name, prompts, score_prompts, sections)\n    save_results(session, results)\n    print(\"Results inserted successfully.\")\n    print_compliance_score(results)\n\n# Run the script\nmain()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9d0e6a8-cc29-4928-a409-b9e4db398531",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\nimport pandas as pd\n\n# Create Snowflake session\nconnection_parameters = {\n    \"account\": \"IW69072\",\n    \"user\": \"AnalyticsInnovators\",\n    \"password\": \"AnalyticsInnovators@123\",\n    \"role\": \"ACCOUNTADMIN\",\n    \"warehouse\": \"COMPUTE_WH\",\n    \"database\": \"SOP_COMPLIANCE\",\n    \"schema\": \"PUBLIC\"\n}\nsession = Session.builder.configs(connection_parameters).create()\n\n# Load data from table\ndf = session.table(\"sop_compliance_results\").to_pandas()\n\n# If no data, exit\nif df.empty:\n    print(\"No data available in sop_compliance_results table.\")\nelse:\n    # Select latest document\n    latest_doc = df.sort_values(\"TIMESTAMP\", ascending=False).iloc[0][\"DOCUMENT_NAME\"]\n    doc_df = df[df[\"DOCUMENT_NAME\"] == latest_doc]\n    timestamp = doc_df[\"TIMESTAMP\"].max()\n\n    # Display header\n    print(f\"üìÑ Document: {latest_doc}\")\n    print(f\"üïí Last Analyzed: {timestamp}\")\n\n    # KPI: Compliance Score\n    total_score = doc_df[\"SCORE\"].sum()\n    max_score = len(doc_df) * 3\n    compliance_score = 100 - int((total_score / max_score) * 100)\n    print(f\"\\n‚úÖ Compliance Score: {100-compliance_score}/100\\n\")\n\n    # Section-wise breakdown\n    print(\"üìå Section Analysis:\\n\")\n    for _, row in doc_df.iterrows():\n        print(f\"üîç Section: {row['SECTION']}\")\n        print(f\"Score: {row['SCORE']} | Severity: {row['SEVERITY']}\")\n        print(f\"AI Response:\\n{row['AI_RESPONSE']}\\n\")\n        print(\"-\" * 80)\n",
   "execution_count": null
  }
 ]
}
